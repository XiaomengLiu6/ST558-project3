---
title: "project3"
author: "Xiaomeng Liu"
date: "2023-11-05"
output:
  github_document:
    toc: true
    toc_depth: 3
    number_sections: false
params:
  ed_level: "4"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# library
```{r, message=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(caret)
library(gam)
library(ModelMetrics)
```

# Introduction section
```{r}
  #per https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf,
  #here are value meanings for the education variable:
    #1 = Never attended school or only kindergarten 
    #2 = Grades 1 through 8 (Elementary)
    #3 = Grades 9 through 11 (Some high school)
    #4 = Grade 12 or GED (High school graduate) 
    #5 = College 1 year to 3 years (Some college or technical school)
    #6 = College 4 years or more (College graduate)
    #9 = Refused

#and here are values for other non-binary variables:
  #income
    #1 = Less than $10,000
    #2 = Less than $15,000 ($10,000 to less than $15,000)
    #3 = Less than $20,000 ($15,000 to less than $20,000)
    #4 = Less than $25,000 ($20,000 to less than $25,000)
    #5 = Less than $35,000 ($25,000 to less than $35,000)
    #6 = Less than $50,000 ($35,000 to less than $50,000)
    #7 = Less than $75,000 ($50,000 to less than $75,000)
    #8 = $75,000 or more
    #77 = Don’t know/Not sure
    #99 = Refused

  #Age
    #1 = Age 18 to 24
    #2 = Age 25 to 29
    #3 = Age 30 to 34
    #4 = Age 35 to 39
    #5 = Age 40 to 44
    #6 = Age 45 to 49
    #7 = Age 50 to 54
    #8 = Age 55 to 59
    #9 = Age 60 to 64
    #10 = Age 65 to 69
    #11 = Age 70 to 74
    #12 = Age 75 to 79
    #13 = Age 80 or older
    #14 = Don’t know/Refused/Missing

  #GenHlth
    #1 = excellent 
    #2 = very good 
    #3 = good 
    #4 = fair 
    #5 = poor 

  #Sex is coded as 0 = female, 1 = male
```

# Data

### Read in the data
```{r}
#read in csv file
diabetes<-read.csv("C:\\Users\\elauf\\Desktop\\ST558\\topic3\\project3-2023-11-08\\diabetes_binary_health_indicators_BRFSS2015.csv")
```

### Combine 1 and 2 Education levels
```{r}
#create collapsed version of education variable
diabetes$Education <- recode(diabetes$Education, `1` = 2)
table(diabetes$Education)
```

### Convert variables to factors
```{r}
#create factor version of variables, where applicable
#in order to facilitate the EDA, for now at least, we'll retain both the factor version and the numeric version of each variable (we may need to drop one version, though, before running models)
diabetes$Diabetes_binary_f	 <- as.factor(diabetes$Diabetes_binary)
diabetes$HighBP_f	 <- as.factor(diabetes$HighBP)
diabetes$HighChol_f	 <- as.factor(diabetes$HighChol)
diabetes$CholCheck_f	 <- as.factor(diabetes$CholCheck)
#BMI -- this is a continuous var
diabetes$Smoker_f	 <- as.factor(diabetes$Smoker)
diabetes$Stroke_f	 <- as.factor(diabetes$Stroke)
diabetes$HeartDiseaseorAttack_f	 <- as.factor(diabetes$HeartDiseaseorAttack)
diabetes$PhysActivity_f	 <- as.factor(diabetes$PhysActivity)
diabetes$Fruits_f	 <- as.factor(diabetes$Fruits)
diabetes$Veggies_f	 <- as.factor(diabetes$Veggies)
diabetes$HvyAlcoholConsump_f	 <- as.factor(diabetes$HvyAlcoholConsump)
diabetes$AnyHealthcare_f	 <- as.factor(diabetes$AnyHealthcare)
diabetes$NoDocbcCost_f	 <- as.factor(diabetes$NoDocbcCost)
diabetes$GenHlth_f <-  as.factor(diabetes$GenHlth)
#MentHlth -- this is a continuous var
#PhysHlth -- this is a continuous var
diabetes$DiffWalk_f	 <- as.factor(diabetes$DiffWalk)
diabetes$Sex_f	 <- as.factor(diabetes$Sex)
diabetes$Age_f	 <- as.factor(diabetes$Age)
diabetes$Education_f	 <- as.factor(diabetes$Education)
diabetes$Income_f	 <- as.factor(diabetes$Income)
```

>
#create factor version of variables, where applicable
#in order to facilitate the EDA, for now at least, we'll retain both the factor version and the numeric version of each variable (we may need to drop one version, though, before running models)
#we've got to jump through some hoops here because the random forest model doesn't like factor variables storing values of 0/1/etc...maybe theres a more elegant way of doing this, Idunno
yn <- function(var, var_f)
{
var_f <- factor(var, levels = make.names(levels(var), unique=TRUE))
}

>                            
yn(var = diabetes$Diabetes_binary, var_f = diabetes$Diabetes_binary_f)
yn(var = diabetes$HighBP, var_f = diabetes$HighBP_f)
yn(var = diabetes$HighChol, var_f = diabetes$HighChol_f)
yn(var = diabetes$CholCheck, var_f = diabetes$CholCheck_f)
#BMI -- this is a continuous var
yn(var = diabetes$Smoker, var_f = diabetes$Smoker_f)
yn(var = diabetes$Stroke, var_f = diabetes$Stroke_f)
yn(var = diabetes$HeartDiseaseorAttack, var_f = diabetes$HeartDiseaseorAttack_f)
yn(var = diabetes$PhysActivity, var_f = diabetes$PhysActivity_f)
yn(var = diabetes$Fruits, var_f = diabetes$Fruits_f)
yn(var = diabetes$Veggies, var_f = diabetes$Veggies_f)
yn(var = diabetes$HvyAlcoholConsump, var_f = diabetes$HvyAlcoholConsump_f)
yn(var = diabetes$AnyHealthcare, var_f = diabetes$AnyHealthcare_f)
yn(var = diabetes$NoDocbcCost, var_f = diabetes$NoDocbcCost_f)
#MentHlth -- this is a continuous var
#PhysHlth -- this is a continuous var
yn(var = diabetes$DiffWalk, var_f = diabetes$DiffWalk_f) 
yn(var = diabetes$GenHlth, var_f = diabetes$GenHlth_f)
yn(var = diabetes$Age, var_f = diabetes$Age_f) 
yn(var = diabetes$Income, var_f = diabetes$Income_f)
yn(var = diabetes$Sex, var_f = diabetes$Sex_f)
yn(var = diabetes$Education, var_f = diabetes$Education_f)


```{r}
#subset to specific level of education per *params* setting
temp <- subset(diabetes, Education==params$ed_level)
```

# Summarizations

```{r}
#confirm that we're working with the desired set of cases
table(temp$Education, temp$Education_f)
ggplot(data=temp, aes(x=Education_f)) + 
  geom_dotplot(binwidth = .05, method = "histodot") + 
  labs(title = "confirm that we're working with the desired set of cases")


#function to check prevalence of diabetes at each level of each factor, and generate corresponding plots
#(I still need to flesh out this function such that it labels the plots)
explore <- function(by_var)
{
results1 <- temp %>%
  group_by({{by_var}}) %>%
  summarize(diabetes_rate = mean(Diabetes_binary))
    #passing variable names to function using curly brackets:
    #https://stackoverflow.com/questions/63433728/how-do-i-pass-a-variable-name-to-an-argument-in-a-function
print(results1)

results2 <- ggplot(data=temp, aes(x={{by_var}}, fill=Diabetes_binary_f)) + 
  geom_bar(stat="count")
print(results2)
}

#probably need to run the above function for at least the sex, age, and income variables, but may not need to run it for this entire list
explore(by_var = HighBP_f)
explore(by_var = HighChol_f)
explore(by_var = CholCheck_f)
explore(by_var = Smoker_f)
explore(by_var = Stroke_f)
explore(by_var = HeartDiseaseorAttack_f)
explore(by_var = PhysActivity_f)
explore(by_var = Fruits_f)
explore(by_var = Veggies_f)
explore(by_var = HvyAlcoholConsump_f)
explore(by_var = AnyHealthcare_f)
explore(by_var = NoDocbcCost_f)
explore(by_var = GenHlth_f)
explore(by_var = DiffWalk_f)
explore(by_var = Sex_f)
explore(by_var = Age_f)
explore(by_var = Income_f)



#correlation matrix (outcome var x continuous vars)
corr_vars <-
  temp %>% select(c(Diabetes_binary, BMI, MentHlth, PhysHlth))
correlation <- cor(corr_vars, method = "spearman")
corrplot(correlation, type = "upper", tl.pos = "lt")
corrplot(correlation, type = "lower", method = "number", add = TRUE, diag = FALSE, tl.pos = "n")



#density plots / boxplots (outcome var x continuous vars)
#I'm guessing we could just choose one or the other
ggplot(data=temp, aes(x=BMI, fill=Diabetes_binary_f)) + 
  geom_density(adjust = 0.5, alpha = 0.5)
ggplot(data=temp, aes(x=Diabetes_binary_f, y=BMI)) + geom_boxplot()

ggplot(data=temp, aes(x=MentHlth, fill=Diabetes_binary_f)) + 
  geom_density(adjust = 0.5, alpha = 0.5)
ggplot(data=temp, aes(x=Diabetes_binary_f, y=MentHlth)) + geom_boxplot()

ggplot(data=temp, aes(x=PhysHlth, fill=Diabetes_binary_f)) + 
  geom_density(adjust = 0.5, alpha = 0.5)
bp3 <- ggplot(data=temp, aes(x=Diabetes_binary_f, y=PhysHlth)) + geom_boxplot()
```

# Modeling

```{r}
#prior to running models, in instances where we have both a factor and a non-factor version of a given variable, we need to first drop the non-factor version of the variable
#we also need to drop both versions of the education variable (since it will not vary given that we've subset our data to a specific education level)
temp$Diabetes_binary	 <- NULL 
temp$HighBP	 <- NULL
temp$HighChol	 <- NULL
temp$CholCheck	 <- NULL
temp$Smoker	 <- NULL
temp$Stroke	 <- NULL
temp$HeartDiseaseorAttack	 <- NULL
temp$PhysActivity	 <- NULL
temp$Fruits	 <- NULL
temp$Veggies	 <- NULL
temp$HvyAlcoholConsump	 <- NULL
temp$AnyHealthcare	 <- NULL
temp$NoDocbcCost	 <- NULL 
temp$GenHlth <-  NULL
temp$DiffWalk	 <- NULL
temp$Sex	 <- NULL
temp$Age	 <- NULL
temp$Education	 <- NULL
temp$Education_f	 <- NULL
temp$Income	 <- NULL

temp$Diabetes_binary_f <- ifelse(temp$Diabetes_binary_f==0, "no", "yes")
temp$Diabetes_binary_f <- as.factor(temp$Diabetes_binary_f)



# set the seed
set.seed(433)
# split the training and testing
indextrain<-createDataPartition(y=temp$Diabetes_binary_f,p=0.7,list=FALSE)
ed_train<-temp[indextrain,]
ed_test<-temp[-indextrain,]
```

#### what log loss is:

The first group member should research and write up a paragraph about what log
loss is and why we may prefer it to things like accuracy.

>
Xiaomeng, I'm sure there is more that can/should be said here, but as best as I can tell, the main gist behind the logLoss metric is to compare actual outcome classification values to predicted probabilities (as opposed to comparing actual outcome classification values to predicted classification values).  Here is a website that seemed to make some sense (to me, at least) when describing the logLoss metric: 
https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a


#### First method: logistic regression

The first group member should provide a reasonably thorough explanation of what a
logistic regression is and why we apply it to this kind of data. Then they should fit
three candidate logistic regression models and choose the best model.

```{r logistic}
ed_logistic<-train(Diabetes_binary_f~.,data=ed_train,
             method="glm", 
             metric="logLoss",
             trControl = trainControl(method = "cv", number = 5, 
                           classProbs = TRUE, summaryFunction = mnLogLoss),
             preProcess=c("center","scale")
)
ed_logistic
```
#### Second method: Lasso logistic
Lasso models aim to leverage the bias-variance trade-off by purposefully introducing small amounts of bias while training the model, with the hope of ultimately decreasing variance (and thereby improving performance when using the model on test data).
PROBABLY NEED TO SAY MORE HERE (this might help: https://www.statology.org/lasso-regression/)

```{r}
lasso_fit <- train(Diabetes_binary_f ~ ., data = ed_train,
  method = "glmnet",
  metric="logLoss",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5, 
                           classProbs = TRUE, summaryFunction = mnLogLoss),
  tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 1, by = 0.1)))

lasso_fit
```


#### Third method: Classification tree

Next we predict the presence of diabetes using a classification tree, in which the predictor space is divided into various "regions", and the predicted value for any given observation is the most common classification among all other observations in that region. Here we train our model using 5-fold cross-validation as well as different tuning parameters to find the optimal number and types of tree splits.
PROBABLY NEED TO SAY MORE HERE

```{r}
#classification tree
ct_fit <- train(Diabetes_binary_f ~ ., data = ed_train,
  method = "rpart",
  metric="logLoss",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5, 
                           classProbs = TRUE, summaryFunction = mnLogLoss),
  tuneGrid = data.frame(cp = seq(from = .001, to = .1, by = .001)))

ct_fit
```


#### Fourth method: Random forest


#### Fifth method: 

new method by Erich
```{r}
loess_fit <- train(Diabetes_binary_f ~ ., data = ed_train,
  method = "gamLoess",
  metric="logLoss",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 5, 
                           classProbs = TRUE, summaryFunction = mnLogLoss),
  tuneGrid = expand.grid(span = seq(0.5, 0.9, len = 5), degree = 1))
#tuning parameters as currently shown are borrowed from https://www.statology.org/loess-regression-in-r/

loess_fit

```

#### Sixth method:

new method by Xiaomeng

# Final Model Selection

You should now have six best models (one for each model type above)

```{r}
choose <- function(in_model)
{
pred <- as.data.frame(predict(in_model, ed_test))
#as.data.frame is used here to avoid error as illustrated here: https://www.statology.org/r-error-operator-is-invalid-for-atomic-vectors/
logLoss(actual = ed_test$Diabetes_binary_f, predicted = pred$fitted.values)
}

choose(in_model = ed_logistic)
choose(in_model = lasso_fit)
choose(in_model = ct_fit)
choose(in_model = loess_fit)
```